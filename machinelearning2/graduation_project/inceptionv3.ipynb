{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(19906)\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from keras.preprocessing import image\n",
    "import cv2\n",
    "import os\n",
    "%matplotlib inline\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, BatchNormalization, Activation\n",
    "from keras.layers.core import Dropout\n",
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "from keras.regularizers import l2\n",
    "import split_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用InceptionV3模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"data/imgs/train2\"\n",
    "val_dir = \"data/imgs/val2\"\n",
    "test_dir = \"data/imgs/test1\"\n",
    "saved_weights = \"saved_weights\"\n",
    "if not os.path.exists(saved_weights):\n",
    "    os.mkdir(saved_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分割验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "drivers_pd = pd.read_csv(\"data/drivers_img_nop081_list.csv\")\n",
    "imgs_pd = drivers_pd[\"img\"]\n",
    "class_pd = drivers_pd[\"classname\"]\n",
    "subject_pd = drivers_pd[\"subject\"]\n",
    "choices = [\"p024\", \"p002\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "选择作为验证集的司机ID: ['p024', 'p002']\n",
      "分割的训练集数量: 20373 ，验证集数量: 1951\n",
      "从增强数据集中分割的训练集数量: 20373\n",
      "link images to directory data/imgs/train2\n",
      "link augmented images to directory data/imgs/train2\n",
      "link images to directory data/imgs/val2\n",
      "split valid data done!\n"
     ]
    }
   ],
   "source": [
    "split_valid.split(choice_ids=choices, \n",
    "                  train_pd_path=\"data/drivers_img_nop081_list.csv\", \n",
    "                  train_aug_pd_path=\"data/drivers_img_aug_list.csv\", \n",
    "                  train_dir=train_dir, \n",
    "                  val_dir=val_dir, \n",
    "                  test_dir=test_dir, \n",
    "                  origin_test_dir=\"data/imgs/test\", \n",
    "                  saved_weights_dir=\"saved_weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_im_cv2(path):\n",
    "    img = cv2.imread(path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "图片预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 图像生成器输出的图像大小\n",
    "out_image_size = (299, 299)\n",
    "batch_size = 96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 40746 images belonging to 10 classes.\n",
      "Found 1951 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "# 训练集图像生成器\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    rotation_range=10.,\n",
    "    width_shift_range=0.05,\n",
    "    height_shift_range=0.05,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    rescale=1./255\n",
    ")\n",
    "\n",
    "# 验证集图像生成器\n",
    "val_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=out_image_size,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    class_mode='categorical')\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=out_image_size,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(10, activation='softmax', use_bias=False, kernel_regularizer=l2(0.01))(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions, name=base_model.name)\n",
    "\n",
    "op = Adam(lr=0.00005, decay=2e-8)\n",
    "model.compile(optimizer=op, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model name: inception_v3 , will save weight file: saved_weights/inception_v3_model.h5\n",
      "WARNING:tensorflow:Variable *= will be deprecated. Use variable.assign_mul if you want assignment to the variable value or 'x = x * y' if you want a new python Tensor object.\n",
      "Epoch 1/20\n",
      "425/425 [==============================] - 1101s 3s/step - loss: 0.5540 - acc: 0.8888 - val_loss: 0.2646 - val_acc: 0.9769\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.26456, saving model to saved_weights/inception_v3_model.h5\n",
      "Epoch 2/20\n",
      "425/425 [==============================] - 1030s 2s/step - loss: 0.1946 - acc: 0.9944 - val_loss: 0.2528 - val_acc: 0.9708\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.26456 to 0.25283, saving model to saved_weights/inception_v3_model.h5\n",
      "Epoch 3/20\n",
      "425/425 [==============================] - 1038s 2s/step - loss: 0.1534 - acc: 0.9974 - val_loss: 0.2762 - val_acc: 0.9431\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.25283\n",
      "Epoch 4/20\n",
      "425/425 [==============================] - 1027s 2s/step - loss: 0.1187 - acc: 0.9981 - val_loss: 0.1847 - val_acc: 0.9646\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.25283 to 0.18466, saving model to saved_weights/inception_v3_model.h5\n",
      "Epoch 5/20\n",
      "425/425 [==============================] - 1030s 2s/step - loss: 0.0867 - acc: 0.9991 - val_loss: 0.1302 - val_acc: 0.9831\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.18466 to 0.13019, saving model to saved_weights/inception_v3_model.h5\n",
      "Epoch 6/20\n",
      "425/425 [==============================] - 1054s 2s/step - loss: 0.0618 - acc: 0.9994 - val_loss: 0.1204 - val_acc: 0.9805\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.13019 to 0.12042, saving model to saved_weights/inception_v3_model.h5\n",
      "Epoch 7/20\n",
      "425/425 [==============================] - 1019s 2s/step - loss: 0.0464 - acc: 0.9990 - val_loss: 0.2858 - val_acc: 0.9088\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.12042\n",
      "Epoch 8/20\n",
      "425/425 [==============================] - 1024s 2s/step - loss: 0.0348 - acc: 0.9991 - val_loss: 0.2177 - val_acc: 0.9359\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.12042\n",
      "Epoch 9/20\n",
      "425/425 [==============================] - 1027s 2s/step - loss: 0.0276 - acc: 0.9990 - val_loss: 0.0824 - val_acc: 0.9780\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.12042 to 0.08245, saving model to saved_weights/inception_v3_model.h5\n",
      "Epoch 10/20\n",
      "425/425 [==============================] - 1026s 2s/step - loss: 0.0257 - acc: 0.9985 - val_loss: 0.0668 - val_acc: 0.9872\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.08245 to 0.06683, saving model to saved_weights/inception_v3_model.h5\n",
      "Epoch 11/20\n",
      "425/425 [==============================] - 1031s 2s/step - loss: 0.0201 - acc: 0.9991 - val_loss: 0.0381 - val_acc: 0.9944\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.06683 to 0.03811, saving model to saved_weights/inception_v3_model.h5\n",
      "Epoch 12/20\n",
      "425/425 [==============================] - 1031s 2s/step - loss: 0.0203 - acc: 0.9988 - val_loss: 0.0330 - val_acc: 0.9954\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.03811 to 0.03300, saving model to saved_weights/inception_v3_model.h5\n",
      "Epoch 13/20\n",
      "425/425 [==============================] - 1010s 2s/step - loss: 0.0174 - acc: 0.9992 - val_loss: 0.1358 - val_acc: 0.9646\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.03300\n",
      "Epoch 14/20\n",
      "425/425 [==============================] - 1033s 2s/step - loss: 0.0187 - acc: 0.9987 - val_loss: 0.0316 - val_acc: 0.9954\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.03300 to 0.03157, saving model to saved_weights/inception_v3_model.h5\n",
      "Epoch 15/20\n",
      "425/425 [==============================] - 1015s 2s/step - loss: 0.0148 - acc: 0.9995 - val_loss: 0.0328 - val_acc: 0.9949\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.03157\n",
      "Epoch 16/20\n",
      "425/425 [==============================] - 1033s 2s/step - loss: 0.0150 - acc: 0.9991 - val_loss: 0.0884 - val_acc: 0.9795\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.03157\n",
      "Epoch 17/20\n",
      "425/425 [==============================] - 1032s 2s/step - loss: 0.0148 - acc: 0.9993 - val_loss: 0.0248 - val_acc: 0.9959\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.03157 to 0.02483, saving model to saved_weights/inception_v3_model.h5\n",
      "Epoch 18/20\n",
      "425/425 [==============================] - 1030s 2s/step - loss: 0.0149 - acc: 0.9990 - val_loss: 0.0467 - val_acc: 0.9887\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.02483\n",
      "Epoch 19/20\n",
      "327/425 [======================>.......] - ETA: 3:55 - loss: 0.0163 - acc: 0.9986"
     ]
    }
   ],
   "source": [
    "# 训练代数\n",
    "epochs = 20\n",
    "\n",
    "save_file = saved_weights + '/' + model.name + '_model.h5'\n",
    "print(\"model name:\", model.name, \", will save weight file:\", save_file)\n",
    "callbacks = [\n",
    "    ModelCheckpoint(save_file, monitor=\"val_loss\", mode=\"min\", save_best_only=True, verbose=1, period=1),\n",
    "    EarlyStopping(monitor=\"val_loss\", verbose=1, mode=\"min\", min_delta=0.0005, patience=3)\n",
    "]\n",
    "\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=val_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "绘制图型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 绘制图型\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Training and validation loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Training and validation acc')\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "pred_model = load_model(\"saved_weights/inception_v3_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预测所有测试集，并生成提交kaggle的报告"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 79726 images belonging to 1 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'test/img_1.jpg'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input, rescale=1./255)\n",
    "pred_batch_size=128\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(test_dir, out_image_size, shuffle=False, \n",
    "                                             batch_size=pred_batch_size, class_mode=None)\n",
    "\n",
    "test_generator.filenames[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "623/623 [==============================] - 1700s 3s/step\n",
      "y_pred shape (79726, 10)\n",
      "              img        c0     c1        c2        c3     c4     c5     c6  \\\n",
      "0       img_1.jpg  0.005000  0.005  0.005000  0.005000  0.005  0.995  0.005   \n",
      "1      img_10.jpg  0.005000  0.005  0.005000  0.005000  0.005  0.995  0.005   \n",
      "2     img_100.jpg  0.976096  0.005  0.005000  0.006738  0.005  0.005  0.005   \n",
      "3    img_1000.jpg  0.005809  0.005  0.050625  0.005000  0.005  0.005  0.005   \n",
      "4  img_100000.jpg  0.005000  0.005  0.005000  0.995000  0.005  0.005  0.005   \n",
      "\n",
      "      c7        c8     c9  \n",
      "0  0.005  0.005000  0.005  \n",
      "1  0.005  0.005000  0.005  \n",
      "2  0.005  0.005000  0.005  \n",
      "3  0.005  0.918554  0.005  \n",
      "4  0.005  0.005000  0.005  \n",
      "predict done.\n"
     ]
    }
   ],
   "source": [
    "sub_df = pd.read_csv(\"data/sample_submission.csv\")\n",
    "\n",
    "y_preds = pred_model.predict_generator(test_generator, verbose=1)\n",
    "y_preds = y_preds.clip(min=0.005, max=0.995)\n",
    "print(\"y_pred shape {}\".format(y_preds.shape))\n",
    "\n",
    "for i, fname in enumerate(test_generator.filenames):\n",
    "    y_pred = y_preds[i]\n",
    "    for k, c in enumerate(y_pred):\n",
    "        sub_df.at[i, 'c'+str(k)] = c\n",
    "\n",
    "print(sub_df.head())\n",
    "\n",
    "sub_df.to_csv('data/pred.csv', index=None)\n",
    "print(\"predict done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提交到kaggle中后得到成绩：private:  0.31309, public: 0.35854"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
